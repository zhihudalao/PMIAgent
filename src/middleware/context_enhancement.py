"""ä¸Šä¸‹æ–‡å¢å¼ºä¸­é—´ä»¶ - å¢å¼ºå¯¹è¯ä¸Šä¸‹æ–‡å’Œæç¤ºè¯è´¨é‡"""

import json
import os
import re
from collections.abc import Awaitable, Callable
from pathlib import Path
from typing import TYPE_CHECKING, Any, Dict, List, Optional

if TYPE_CHECKING:
    from langgraph.runtime import Runtime

from deepagents.backends.protocol import BackendProtocol
from langchain.agents.middleware.types import (AgentMiddleware, AgentState,
                                               ModelRequest, ModelResponse)
from typing_extensions import NotRequired, TypedDict


class ContextEnhancementState(AgentState):
    """ä¸Šä¸‹æ–‡å¢å¼ºä¸­é—´ä»¶çš„çŠ¶æ€"""

    enhanced_context: NotRequired[Dict[str, Any]]
    """å¢å¼ºçš„ä¸Šä¸‹æ–‡ä¿¡æ¯"""

    project_info: NotRequired[Dict[str, Any]]
    """é¡¹ç›®ä¿¡æ¯"""

    user_preferences: NotRequired[Dict[str, Any]]
    """ç”¨æˆ·åå¥½"""

    conversation_context: NotRequired[Dict[str, Any]]
    """å¯¹è¯ä¸Šä¸‹æ–‡"""


class ContextEnhancementMiddleware(AgentMiddleware):
    """ä¸Šä¸‹æ–‡å¢å¼ºä¸­é—´ä»¶

    ä¸ºAIæ¨¡å‹æä¾›ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š
    - é¡¹ç›®ç»“æ„å’Œæ–‡ä»¶åˆ†æ
    - ç”¨æˆ·åå¥½å’Œå†å²æ¨¡å¼
    - å¯¹è¯ä¸Šä¸‹æ–‡å¢å¼º
    - ä»£ç è´¨é‡å’Œæœ€ä½³å®è·µæç¤º
    """

    state_schema = ContextEnhancementState

    def __init__(
        self,
        *,
        backend: BackendProtocol,
        context_path: str = "/context/",
        enable_project_analysis: bool = True,
        enable_user_preferences: bool = True,
        enable_conversation_enhancement: bool = True,
        max_context_length: int = 4000,
    ) -> None:
        """åˆå§‹åŒ–ä¸Šä¸‹æ–‡å¢å¼ºä¸­é—´ä»¶"""
        self.backend = backend
        self.context_path = context_path.rstrip("/") + "/"
        self.enable_project_analysis = enable_project_analysis
        self.enable_user_preferences = enable_user_preferences
        self.enable_conversation_enhancement = enable_conversation_enhancement
        self.max_context_length = max_context_length

    def _analyze_project_structure(self, workspace_path: str) -> Dict[str, Any]:
        """åˆ†æé¡¹ç›®ç»“æ„"""
        if not self.enable_project_analysis:
            return {}

        try:
            path = Path(workspace_path)
            if not path.exists():
                return {}

            project_info = {
                "name": path.name,
                "path": str(path),
                "type": self._detect_project_type(path),
                "languages": self._detect_programming_languages(path),
                "frameworks": self._detect_frameworks(path),
                "key_files": self._get_key_files(path),
                "recent_files": self._get_recent_files(path),
                "project_stats": self._get_project_stats(path),
            }

            return project_info
        except Exception as e:
            print(f"Warning: Project analysis failed: {e}")
            return {}

    def _detect_project_type(self, path: Path) -> str:
        """æ£€æµ‹é¡¹ç›®ç±»å‹"""
        indicators = {
            "web": ["package.json", "requirements.txt", "composer.json", "Gemfile"],
            "mobile": ["Podfile", "build.gradle", "Info.plist", "AndroidManifest.xml"],
            "desktop": ["CMakeLists.txt", "Cargo.toml", "setup.py", "pom.xml"],
            "data_science": ["requirements.txt", "environment.yml", "Dockerfile"],
            "game": ["main.js", "index.html", "manifest.xml"],
        }

        files = [f.name for f in path.iterdir() if f.is_file()]

        for project_type, indicator_files in indicators.items():
            if any(indicator in files for indicator in indicator_files):
                return project_type

        return "general"

    def _detect_programming_languages(self, path: Path) -> List[str]:
        """æ£€æµ‹ä½¿ç”¨çš„ç¼–ç¨‹è¯­è¨€"""
        language_extensions = {
            "Python": [".py"],
            "JavaScript": [".js", ".mjs", ".cjs"],
            "TypeScript": [".ts"],
            "Java": [".java"],
            "C++": [".cpp", ".cc", ".cxx"],
            "C": [".c"],
            "Go": [".go"],
            "Rust": [".rs"],
            "Ruby": [".rb"],
            "PHP": [".php"],
            "C#": [".cs"],
            "Swift": [".swift"],
            "Kotlin": [".kt"],
            "HTML": [".html", ".htm"],
            "CSS": [".css", ".scss", ".sass"],
        }

        detected_languages = set()

        try:
            for file_path in path.rglob("*"):
                if file_path.is_file():
                    for lang, extensions in language_extensions.items():
                        if any(str(file_path).endswith(ext) for ext in extensions):
                            detected_languages.add(lang)
        except Exception:
            pass

        return list(detected_languages)

    def _detect_frameworks(self, path: Path) -> List[str]:
        """æ£€æµ‹ä½¿ç”¨çš„æ¡†æ¶"""
        frameworks = {
            "React": ["package.json", "react"],
            "Vue": ["package.json", "vue"],
            "Angular": ["package.json", "angular"],
            "Django": ["settings.py", "django"],
            "Flask": ["app.py", "flask"],
            "FastAPI": ["main.py", "fastapi"],
            "Express": ["package.json", "express"],
            "Next.js": ["next.config.js", "next"],
            "TensorFlow": ["requirements.txt", "tensorflow"],
            "PyTorch": ["requirements.txt", "torch"],
        }

        detected_frameworks = []

        try:
            for file_path in path.rglob("*.json"):
                if file_path.is_file():
                    content = file_path.read_text(encoding="utf-8").lower()
                    for framework, (filename, keyword) in frameworks.items():
                        if file_path.name == filename and keyword in content:
                            detected_frameworks.append(framework)

            # æ£€æŸ¥Pythonæ–‡ä»¶ä¸­çš„æ¡†æ¶
            for file_path in path.rglob("*.py"):
                if file_path.is_file():
                    content = file_path.read_text(encoding="utf-8").lower()
                    for framework, (filename, keyword) in frameworks.items():
                        if filename == "settings.py" and keyword in content:
                            detected_frameworks.append(framework)
                        elif keyword in content and framework in [
                            "Django",
                            "Flask",
                            "FastAPI",
                        ]:
                            detected_frameworks.append(framework)

        except Exception:
            pass

        return detected_frameworks

    def _get_key_files(self, path: Path) -> List[str]:
        """è·å–å…³é”®æ–‡ä»¶"""
        key_patterns = [
            "README*",
            "LICENSE*",
            "*.md",
            "*.txt",
            "Dockerfile",
            "Makefile",
            ".gitignore",
            "requirements.txt",
            "package.json",
            "pyproject.toml",
            "setup.py",
            "Cargo.toml",
            "pom.xml",
            "build.gradle",
        ]

        key_files = []
        try:
            for pattern in key_patterns:
                for file_path in path.glob(pattern):
                    if file_path.is_file():
                        key_files.append(file_path.name)
        except Exception:
            pass

        return key_files[:20]  # é™åˆ¶æ•°é‡

    def _get_recent_files(self, path: Path) -> List[str]:
        """è·å–æœ€è¿‘ä¿®æ”¹çš„æ–‡ä»¶"""
        recent_files = []
        try:
            files = []
            for file_path in path.rglob("*"):
                if file_path.is_file() and not file_path.name.startswith("."):
                    try:
                        mtime = file_path.stat().st_mtime
                        files.append((mtime, str(file_path.relative_to(path))))
                    except OSError:
                        continue

            files.sort(reverse=True)
            recent_files = [file for _, file in files[:10]]
        except Exception:
            pass

        return recent_files

    def _get_project_stats(self, path: Path) -> Dict[str, Any]:
        """è·å–é¡¹ç›®ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        try:
            file_count = 0
            total_size = 0
            file_types = {}

            for file_path in path.rglob("*"):
                if file_path.is_file():
                    file_count += 1
                    try:
                        size = file_path.stat().st_size
                        total_size += size

                        ext = file_path.suffix.lower()
                        if ext:
                            file_types[ext] = file_types.get(ext, 0) + 1
                    except OSError:
                        continue

            stats = {
                "file_count": file_count,
                "total_size_mb": round(total_size / (1024 * 1024), 2),
                "file_types": dict(
                    sorted(file_types.items(), key=lambda x: x[1], reverse=True)[:10]
                ),
            }
        except Exception:
            pass

        return stats

    def _analyze_conversation_patterns(self, messages: List[Dict]) -> Dict[str, Any]:
        """åˆ†æå¯¹è¯æ¨¡å¼"""
        if not self.enable_conversation_enhancement:
            return {}

        try:
            patterns = {
                "user_intent": self._detect_user_intent(messages),
                "technical_level": self._assess_technical_level(messages),
                "preferred_response_length": self._analyze_response_preferences(
                    messages
                ),
                "topic_keywords": self._extract_keywords(messages),
                "conversation_style": self._detect_conversation_style(messages),
            }

            return patterns
        except Exception:
            return {}

    def _detect_user_intent(self, messages: List[Dict]) -> str:
        """æ£€æµ‹ç”¨æˆ·æ„å›¾"""
        if not messages:
            return "general"

        user_messages = [
            msg
            for msg in messages
            if (hasattr(msg, "type") and msg.type == "human")
            or (hasattr(msg, "get") and msg.get("role") == "user")
        ]
        if not user_messages:
            return "general"

        last_message = (
            user_messages[-1].content
            if hasattr(user_messages[-1], "content")
            else user_messages[-1].get("content", "")
        ).lower()

        intent_keywords = {
            "coding": ["code", "code", "code", "function", "å‡½æ•°", "ä»£ç ", "ç¼–ç¨‹"],
            "analysis": ["analyze", "analysis", "åˆ†æ", "review", "review"],
            "fixing": ["fix", "error", "bug", "ä¿®å¤", "é”™è¯¯", "é—®é¢˜"],
            "learning": ["learn", "explain", "å­¦ä¹ ", "è§£é‡Š", "æ•™æˆ‘"],
            "question": ["?", "ï¼Ÿ", "how", "what", "why", "å¦‚ä½•", "ä»€ä¹ˆ", "ä¸ºä»€ä¹ˆ"],
        }

        for intent, keywords in intent_keywords.items():
            if any(keyword in last_message for keyword in keywords):
                return intent

        return "general"

    def _assess_technical_level(self, messages: List[Dict]) -> str:
        """è¯„ä¼°æŠ€æœ¯æ°´å¹³"""
        if not messages:
            return "intermediate"

        all_content = " ".join(
            [
                (msg.content if hasattr(msg, "content") else msg.get("content", ""))
                for msg in messages
                if (hasattr(msg, "type") and msg.type in ["human", "ai"])
                or (hasattr(msg, "get") and msg.get("role") in ["user", "assistant"])
            ]
        ).lower()

        beginner_keywords = ["new", "beginner", "åˆå­¦è€…", "æ–°æ‰‹", "å…¥é—¨", "ç®€å•"]
        advanced_keywords = [
            "optimization",
            "architecture",
            "architecture",
            "performance",
            "ä¼˜åŒ–",
            "æ¶æ„",
            "é«˜çº§",
        ]

        if any(keyword in all_content for keyword in beginner_keywords):
            return "beginner"
        elif any(keyword in all_content for keyword in advanced_keywords):
            return "advanced"

        return "intermediate"

    def _analyze_response_preferences(self, messages: List[Dict]) -> str:
        """åˆ†æå“åº”åå¥½"""
        assistant_messages = [
            msg
            for msg in messages
            if (hasattr(msg, "type") and msg.type == "ai")
            or (hasattr(msg, "get") and msg.get("role") == "assistant")
        ]
        if len(assistant_messages) < 2:
            return "medium"

        avg_length = sum(
            len(msg.content if hasattr(msg, "content") else msg.get("content", ""))
            for msg in assistant_messages
        ) / len(assistant_messages)

        if avg_length < 200:
            return "short"
        elif avg_length > 800:
            return "detailed"

        return "medium"

    def _extract_keywords(self, messages: List[Dict]) -> List[str]:
        """æå–å…³é”®è¯"""
        if not messages:
            return []

        # æŠ€æœ¯å…³é”®è¯åˆ—è¡¨
        tech_keywords = [
            "python",
            "javascript",
            "java",
            "react",
            "vue",
            "angular",
            "docker",
            "git",
            "api",
            "database",
            "frontend",
            "backend",
            "devops",
            "testing",
            "bug",
            "feature",
            "refactor",
            "optimize",
            "deploy",
            "security",
        ]

        all_content = " ".join(
            [
                (msg.content if hasattr(msg, "content") else msg.get("content", ""))
                for msg in messages
                if (hasattr(msg, "type") and msg.type in ["human", "ai"])
                or (hasattr(msg, "get") and msg.get("role") in ["user", "assistant"])
            ]
        ).lower()

        found_keywords = [
            keyword for keyword in tech_keywords if keyword in all_content
        ]

        return found_keywords[:10]  # é™åˆ¶æ•°é‡

    def _detect_conversation_style(self, messages: List[Dict]) -> str:
        """æ£€æµ‹å¯¹è¯é£æ ¼"""
        user_messages = [
            msg
            for msg in messages
            if (hasattr(msg, "type") and msg.type == "human")
            or (hasattr(msg, "get") and msg.get("role") == "user")
        ]
        if not user_messages:
            return "professional"

        last_message = (
            user_messages[-1].content
            if hasattr(user_messages[-1], "content")
            else user_messages[-1].get("content", "")
        )

        informal_indicators = ["!", "å“ˆ", "å‘µå‘µ", "å“ˆå“ˆ", "ğŸ˜Š", "ğŸ‘", "è°¢è°¢"]
        formal_indicators = ["è¯·", "è¯·é—®", "èƒ½å¦", "å¯å¦", "æ„Ÿè°¢"]

        informal_count = sum(
            1 for indicator in informal_indicators if indicator in last_message
        )
        formal_count = sum(
            1 for indicator in formal_indicators if indicator in last_message
        )

        if informal_count > formal_count:
            return "casual"
        elif formal_count > informal_count:
            return "formal"

        return "professional"

    async def _build_context_enhancement(self, request: ModelRequest) -> str:
        """æ„å»ºä¸Šä¸‹æ–‡å¢å¼ºä¿¡æ¯"""
        context_parts = []

        # 1. é¡¹ç›®ä¿¡æ¯
        if self.enable_project_analysis:
            import asyncio
            workspace = await asyncio.to_thread(os.getcwd)
            project_info = await asyncio.to_thread(self._analyze_project_structure, workspace)
            if project_info:
                context_parts.append("## é¡¹ç›®ä¸Šä¸‹æ–‡")
                context_parts.append(f"- é¡¹ç›®ç±»å‹: {project_info.get('type', 'æœªçŸ¥')}")
                context_parts.append(
                    f"- ç¼–ç¨‹è¯­è¨€: {', '.join(project_info.get('languages', []))}"
                )
                context_parts.append(
                    f"- æ£€æµ‹æ¡†æ¶: {', '.join(project_info.get('frameworks', []))}"
                )
                context_parts.append(
                    f"- æ–‡ä»¶ç»Ÿè®¡: {project_info.get('project_stats', {}).get('file_count', 0)} ä¸ªæ–‡ä»¶"
                )

        # 2. å¯¹è¯æ¨¡å¼
        if self.enable_conversation_enhancement:
            messages = request.state.get("messages", [])
            conversation_patterns = self._analyze_conversation_patterns(messages)
            if conversation_patterns:
                context_parts.append("\n## å¯¹è¯ä¸Šä¸‹æ–‡")
                context_parts.append(
                    f"- ç”¨æˆ·æ„å›¾: {conversation_patterns.get('user_intent', 'general')}"
                )
                context_parts.append(
                    f"- æŠ€æœ¯æ°´å¹³: {conversation_patterns.get('technical_level', 'intermediate')}"
                )
                context_parts.append(
                    f"- å“åº”åå¥½: {conversation_patterns.get('preferred_response_length', 'medium')}"
                )

                keywords = conversation_patterns.get("topic_keywords", [])
                if keywords:
                    context_parts.append(f"- ç›¸å…³æŠ€æœ¯: {', '.join(keywords)}")

        # 3. å¢å¼ºå»ºè®®
        context_parts.append("\n## ä¸Šä¸‹æ–‡å¢å¼ºå»ºè®®")
        context_parts.append("- åŸºäºé¡¹ç›®ç±»å‹å’Œç”¨æˆ·æ„å›¾ï¼Œæä¾›é’ˆå¯¹æ€§çš„æŠ€æœ¯å»ºè®®")
        context_parts.append("- æ ¹æ®æŠ€æœ¯æ°´å¹³è°ƒæ•´è§£é‡Šçš„è¯¦ç»†ç¨‹åº¦")
        context_parts.append("- è€ƒè™‘ç”¨æˆ·çš„å“åº”åå¥½ï¼Œè°ƒæ•´å›ç­”é•¿åº¦")

        return "\n".join(context_parts)

    def before_agent(
        self,
        state: ContextEnhancementState,
        runtime,
    ) -> ContextEnhancementState:
        """åœ¨ä»£ç†æ‰§è¡Œå‰åˆå§‹åŒ–ä¸Šä¸‹æ–‡å¢å¼º"""
        enhanced_context = {
            "project_analyzed": self.enable_project_analysis,
            "user_preferences_enabled": self.enable_user_preferences,
            "conversation_enhancement_enabled": self.enable_conversation_enhancement,
            "initialized_at": time.time(),
        }

        return {
            "enhanced_context": enhanced_context,
            "project_info": {},
            "user_preferences": {},
            "conversation_context": {},
        }

    async def abefore_agent(
        self,
        state: ContextEnhancementState,
        runtime,
    ) -> ContextEnhancementState:
        """å¼‚æ­¥ï¼šåœ¨ä»£ç†æ‰§è¡Œå‰åˆå§‹åŒ–ä¸Šä¸‹æ–‡å¢å¼º"""
        return self.before_agent(state, runtime)

    def wrap_model_call(
        self,
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse],
    ) -> ModelResponse:
        """åŒ…è£…æ¨¡å‹è°ƒç”¨ï¼Œæ³¨å…¥å¢å¼ºçš„ä¸Šä¸‹æ–‡"""

        # æ„å»ºä¸Šä¸‹æ–‡å¢å¼º
        context_enhancement = self._build_context_enhancement(request)

        # æ³¨å…¥åˆ°ç³»ç»Ÿæç¤ºä¸­
        if context_enhancement:
            if request.system_prompt:
                request.system_prompt = (
                    f"{context_enhancement}\n\n{request.system_prompt}"
                )
            else:
                request.system_prompt = context_enhancement

        # æ‰§è¡ŒåŸå§‹è¯·æ±‚
        return handler(request)

    async def awrap_model_call(
        self,
        request: ModelRequest,
        handler: Callable[[ModelRequest], Awaitable[ModelResponse]],
    ) -> ModelResponse:
        """å¼‚æ­¥ï¼šåŒ…è£…æ¨¡å‹è°ƒç”¨ï¼Œæ³¨å…¥å¢å¼ºçš„ä¸Šä¸‹æ–‡"""

        # æ„å»ºä¸Šä¸‹æ–‡å¢å¼º
        context_enhancement = await self._build_context_enhancement(request)

        # æ³¨å…¥åˆ°ç³»ç»Ÿæç¤ºä¸­
        if context_enhancement:
            if request.system_prompt:
                request.system_prompt = (
                    f"{context_enhancement}\n\n{request.system_prompt}"
                )
            else:
                request.system_prompt = context_enhancement

        # æ‰§è¡ŒåŸå§‹è¯·æ±‚
        return await handler(request)


# æ·»åŠ ç¼ºå°‘çš„import
import time
